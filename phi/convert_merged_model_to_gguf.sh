#!/bin/bash
# ê°œì„ ëœ GGUF ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ (ëª¨ë“  ë¬¸ì œ í•´ê²°ëœ ìµœì¢… ë²„ì „)

set -e # ì—ëŸ¬ ë°œìƒ ì‹œ ìŠ¤í¬ë¦½íŠ¸ ì¤‘ë‹¨

echo "ğŸš€ GGUF ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì‹œì‘"
echo "================================"

# --- í•„ìˆ˜ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜ (cmake, libcurl-dev) ---
echo "âœ… í•„ìˆ˜ íŒ¨í‚¤ì§€(cmake, libcurl-dev) ì„¤ì¹˜ í™•ì¸ ë° ì‹œë„..."

# apt (Debian/Ubuntu) ë˜ëŠ” yum/dnf (CentOS/RHEL) í™•ì¸
if command -v apt-get &> /dev/null; then
    echo "ğŸ“¦ apt íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ê°ì§€. cmakeì™€ libcurl4-openssl-dev ì„¤ì¹˜ ì¤‘..."
    apt-get update
    apt-get install -y cmake libcurl4-openssl-dev
elif command -v yum &> /dev/null; then
    echo "ğŸ“¦ yum íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ê°ì§€. cmakeì™€ libcurl-devel ì„¤ì¹˜ ì¤‘..."
    yum install -y cmake libcurl-devel
elif command -v dnf &> /dev/null; then
    echo "ğŸ“¦ dnf íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ê°ì§€. cmakeì™€ libcurl-devel ì„¤ì¹˜ ì¤‘..."
    dnf install -y cmake libcurl-devel
else
    echo "âš ï¸ ì§€ì›ë˜ëŠ” íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €(apt, yum, dnf)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    echo "cmakeì™€ libcurl-devel(ë˜ëŠ” libcurl4-openssl-dev)ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
fi
echo "âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜/í™•ì¸ ì™„ë£Œ."
# --- ì„¤ì¹˜ ë¡œì§ ë ---


# ëª¨ë¸ ê²½ë¡œ ìë™ ê°ì§€ ë˜ëŠ” ìˆ˜ë™ ì„¤ì •
if [ -z "$1" ]; then
    MODEL_PATH=$(find ./out -mindepth 1 -maxdepth 1 -type d -printf '%T@ %p\n' | sort -nr | head -1 | cut -d' ' -f2-)
    if [ -z "$MODEL_PATH" ]; then
        echo "âŒ ./out ë””ë ‰í† ë¦¬ì—ì„œ í•™ìŠµ ê²°ê³¼ë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        exit 1
    fi
    echo "âœ… ê°€ì¥ ìµœê·¼ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ê°ì§€í–ˆìŠµë‹ˆë‹¤."
else
    MODEL_PATH="$1"
    echo "âœ… ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í–ˆìŠµë‹ˆë‹¤."
fi

echo "ğŸ“ ëª¨ë¸ ê²½ë¡œ: $MODEL_PATH"

GGUF_DIR="./gguf_models"
mkdir -p "$GGUF_DIR"

if [ ! -d "llama.cpp" ]; then
    echo "ğŸ“¥ llama.cpp í´ë¡  ì¤‘..."
    git clone https://github.com/ggerganov/llama.cpp.git
fi

cd llama.cpp
echo "ğŸ”„ llama.cpp ì—…ë°ì´íŠ¸ ì¤‘..."
git pull

echo "ğŸ”¨ llama.cpp ë¹Œë“œ ì¤‘ (CMake, CUDA ì§€ì›)..."
rm -rf build
if command -v nvcc &> /dev/null; then
    echo "ğŸš€ CUDA ê°ì§€ - GPU ê°€ì† ë¹Œë“œ"
    cmake -B build -DLLAMA_CUDA=ON
else
    echo "âš ï¸ CUDA ë¯¸ê°ì§€ - CPU ì „ìš© ë¹Œë“œ"
    cmake -B build
fi
cmake --build build --config Release -j $(nproc)

echo "ğŸ“¦ Python ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘..."
pip install -q -r requirements.txt

ABS_MODEL_PATH=$(cd ..; realpath "$MODEL_PATH")
if [ ! -f "$ABS_MODEL_PATH/config.json" ]; then
    echo "âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë¸ ë””ë ‰í† ë¦¬: $ABS_MODEL_PATH"
    exit 1
fi

MODEL_NAME=$(basename "$ABS_MODEL_PATH")
OUTPUT_NAME="${MODEL_NAME}-finetuned"

echo "ğŸ”„ HuggingFace â†’ GGUF ë³€í™˜ ì¤‘..."
# --- ìˆ˜ì •ëœ ë¶€ë¶„: convert.py -> convert-hf-to-gguf.py ---
python convert-hf-to-gguf.py "$ABS_MODEL_PATH" \
    --outtype f16 \
    --outfile "../$GGUF_DIR/${OUTPUT_NAME}-f16.gguf"
# --- ìˆ˜ì • ë ---

cd ..
echo "âœ… FP16 ë³€í™˜ ì™„ë£Œ"

cd "$GGUF_DIR"
FP16_SIZE=$(du -h "${OUTPUT_NAME}-f16.gguf" | cut -f1)
echo "ğŸ“ FP16 ëª¨ë¸ í¬ê¸°: $FP16_SIZE"

echo "ğŸ”„ ì–‘ìí™” ì‹œì‘..."
declare -A QUANT_LEVELS=(
    ["q4_k_m"]="4ë¹„íŠ¸, ê· í˜•ì¡íŒ í’ˆì§ˆ (ì¶”ì²œ)"
    ["q5_k_m"]="5ë¹„íŠ¸, ë†’ì€ í’ˆì§ˆ"
)

for level in "${!QUANT_LEVELS[@]}"; do
    output_file="${OUTPUT_NAME}-${level}.gguf"
    echo "    ğŸ”§ $level ì–‘ìí™” ì¤‘... (${QUANT_LEVELS[$level]})"

    ../llama.cpp/build/bin/quantize \
        "${OUTPUT_NAME}-f16.gguf" \
        "$output_file" \
        "$level"

    size=$(du -h "$output_file" | cut -f1)
    echo "    âœ… $output_file ìƒì„± ì™„ë£Œ ($size)"
done

echo ""
echo "ğŸ‰ GGUF ë³€í™˜ ì™„ë£Œ!"
echo "================================"
echo "ìƒì„±ëœ ëª¨ë¸ íŒŒì¼ë“¤:"
ls -lh *.gguf | awk '{print "  " $9 " (" $5 ")"}'
echo ""
echo "ğŸ’¡ ì¶”ì²œ: ${OUTPUT_NAME}-q4_k_m.gguf"
echo ""
tar -czf "${MODEL_NAME}-gguf-models.tar.gz" *.gguf
echo "âœ… ì••ì¶• ì™„ë£Œ: ${MODEL_NAME}-gguf-models.tar.gz"
echo "================================"
