from llama_cpp import Llama
import time

MODEL_PATH = "merged_model-finetuned-q5_k_m.gguf"

print("Loading model for Apple Metal...")
start_time = time.time()

llm = Llama(
    model_path=MODEL_PATH,
    n_gpu_layers=0,  # -1ë¡œ ì„¤ì • ì‹œ, ê°€ëŠ¥í•œ ëª¨ë“  ë ˆì´ì–´ë¥¼ Metal GPUë¡œ ì˜¤í”„ë¡œë”©
    n_ctx=4096,
    verbose=True
)

end_time = time.time()
print(f"Model loaded in {end_time - start_time:.2f} seconds.")
print("-" * 50)

# --- 2. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ---
# íŒŒì¸íŠœë‹ ì‹œ ì‚¬ìš©í–ˆë˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ë§ì¶° ì§ˆë¬¸ì„ êµ¬ì„±í•´ì•¼ ëª¨ë¸ì´ ê°€ì¥ ì˜ ì‘ë‹µí•©ë‹ˆë‹¤.
# (ì˜ˆ: Phi-3 í…œí”Œë¦¿)
user_question = "ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì¤˜."

prompt = f"""<|system|>
You are a helpful AI assistant.<|end|>
<|user|>
{user_question}<|end|>
<|assistant|>"""


# --- 3. í…ìŠ¤íŠ¸ ìƒì„± (ì¶”ë¡ ) ---
print("Generating response...")
start_time = time.time()

output = llm(
    prompt,
    max_tokens=1024,      # ìµœëŒ€ ìƒì„± í† í° ìˆ˜
    temperature=0.7,      # ìƒì„±ì˜ ë‹¤ì–‘ì„±ì„ ì¡°ì ˆ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê²°ì •ì )
    top_p=0.9,            # ìƒì„± í† í°ì˜ í›„ë³´êµ°ì„ ì œí•œ
    stop=["<|end|>"],     # ì‘ë‹µ ìƒì„±ì„ ë©ˆì¶œ íŠ¹ì • í† í° ì§€ì •
    echo=False            # Trueë¡œ ì„¤ì • ì‹œ, ì…ë ¥í•œ í”„ë¡¬í”„íŠ¸ê¹Œì§€ í•¨ê»˜ ì¶œë ¥
)

end_time = time.time()
print(f"Response generated in {end_time - start_time:.2f} seconds.")
print("-" * 50)


# --- 4. ê²°ê³¼ ì¶œë ¥ ---
generated_text = output['choices'][0]['text'].strip()
print("ğŸ¤– ëª¨ë¸ ì‘ë‹µ:\n")
print(generated_text)