import os
import shutil
import warnings
from typing import Optional

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import SFTTrainer, SFTConfig

# =========================
# 0) í™˜ê²½ / í† í°
# =========================
HF_TOKEN: Optional[str] = os.environ.get("HF_TOKEN")

# ìºì‹œ/ê²½ê³  ì†ŒìŒ ì¤„ì´ê¸°
os.environ.setdefault("TRANSFORMERS_NO_ADVISORY_WARNINGS", "1")
warnings.filterwarnings("ignore", category=UserWarning)

# PyTorch matmul ì •ë°€ë„(ì†ë„/ì•ˆì •ì„± ë°¸ëŸ°ìŠ¤)
try:
    torch.set_float32_matmul_precision("high")
except Exception:
    pass

# =========================
# 1) í•˜ì´í¼íŒŒë¼ë¯¸í„°
# =========================
EPOCHS = 3
LR = 2e-4
TRAIN_BS = 1
GRAD_ACCUM = 16
EVAL_BS = 1
PACKING = True
LOG_STEPS = 10
EVAL_STEPS = 50
SAVE_STEPS = 100
SEED = 42
MAX_LENGTH = None  # í•„ìš” ì‹œ 4096 ë“±

USE_LORA = True
LORA_R = 128
LORA_ALPHA = 256
LORA_DROPOUT = 0.05
LORA_TARGETS = "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj"

# =========================
# 2) ê²½ë¡œ/ëª¨ë¸
# =========================
MODEL_ID = "microsoft/Phi-3.5-mini-instruct"

TRAIN_JSONL = "train.jsonl"
DATASET_NAME = "MyDataset"  # ë°ì´í„°ì…‹ íŠ¹ì„±ì— ë§ê²Œ ë³€ê²½

EVAL_JSONL = ""  # ì„ íƒ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
OUTPUT_DIR = f"./out/{MODEL_ID.split('/')[-1]}_lora_adapter_r{LORA_R}_{DATASET_NAME}"

# ë””ë°”ì´ìŠ¤/ì •ë°€ë„
DEVICE_MAP = "auto"
USE_BF16 = torch.cuda.is_available() and DEVICE_MAP != "cpu"

# =========================
# 3) í—ˆê¹…í˜ì´ìŠ¤ ë¡œê·¸ì¸
# =========================
if HF_TOKEN:
    try:
        from huggingface_hub import login as hf_login

        hf_login(token=HF_TOKEN.strip())
        print("âœ… HF ë¡œê·¸ì¸ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ HF ë¡œê·¸ì¸ ì‹¤íŒ¨(ë¬´ì‹œ): {e}")


# =========================
# 4) ë°ì´í„° ë¡œë”
# =========================
def load_and_prepare_jsonl(train_path: str, eval_path: Optional[str], text_field: str = "text"):
    if not os.path.exists(train_path):
        raise FileNotFoundError(f"í•™ìŠµ íŒŒì¼ ì—†ìŒ: {train_path}")

    data_files = {"train": train_path}
    if eval_path:
        if not os.path.exists(eval_path):
            raise FileNotFoundError(f"í‰ê°€ íŒŒì¼ ì—†ìŒ: {eval_path}")
        data_files["validation"] = eval_path

    ds = load_dataset("json", data_files=data_files)
    tr = ds["train"];
    ev = ds.get("validation")

    cols = set(tr.column_names)
    if not ({"instruction", "output"}.issubset(cols) or {"input", "output"}.issubset(cols)):
        raise ValueError(f"ë°ì´í„° ì»¬ëŸ¼ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤ì œ ì»¬ëŸ¼: {tr.column_names}")

    def to_text(ex):
        instr = (ex.get("instruction") or ex.get("input") or "").strip()
        out = (ex.get("output") or "").strip()
        inp = (ex.get("input") if "instruction" in ex else "").strip()

        if not instr and inp and "instruction" not in ex:
            instr, inp = inp, ""

        if not instr or not out: return None

        if inp:
            return {text_field: f"### Instruction:\n{instr}\n\n### Input:\n{inp}\n\n### Response:\n{out}"}
        else:
            return {text_field: f"### Instruction:\n{instr}\n\n### Response:\n{out}"}

    tr = tr.map(to_text, remove_columns=list(cols)).filter(lambda x: x is not None)
    if ev:
        ev = ev.map(to_text, remove_columns=list(cols)).filter(lambda x: x is not None)

    return tr, ev


# =========================
# 5) ë©”ì¸ í•¨ìˆ˜
# =========================
def main():
    # ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ í™•ì¸
    try:
        from huggingface_hub import HfApi
        HfApi().model_info(MODEL_ID, token=HF_TOKEN)
    except Exception as e:
        raise RuntimeError(f"ëª¨ë¸ ë¦¬í¬ í™•ì¸ ì‹¤íŒ¨: {MODEL_ID}\nì›ì¸: {e}")

    # ì‹œë“œ ê³ ì •
    try:
        import random, numpy as np
        random.seed(SEED);
        np.random.seed(SEED);
        torch.manual_seed(SEED)
        if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)
    except Exception:
        pass

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print("\n" + "=" * 50 + "\nğŸš€ LoRA ì–´ëŒ‘í„° í•™ìŠµ ì‹œì‘\n" + "=" * 50)
    print(f"Model ID: {MODEL_ID}\nOutput Dir: {OUTPUT_DIR}\nLoRA r: {LORA_R}")
    print("=" * 50 + "\n")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=True, token=HF_TOKEN)
    if tok.pad_token is None: tok.pad_token = tok.eos_token

    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map=DEVICE_MAP,
        torch_dtype=torch.bfloat16 if USE_BF16 else torch.float32,
        trust_remote_code=True,
        token=HF_TOKEN,
        attn_implementation="eager"
    )

    if torch.cuda.is_available(): model.gradient_checkpointing_enable()
    print("âœ… ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

    # ë°ì´í„° ë¡œë“œ
    train_ds, eval_ds = load_and_prepare_jsonl(TRAIN_JSONL, EVAL_JSONL or None)
    print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ (í•™ìŠµ: {len(train_ds)}ê°œ)")

    # SFT ì„¤ì •
    sft_config = SFTConfig(
        output_dir=OUTPUT_DIR,
        dataset_text_field="text",
        packing=PACKING,
        per_device_train_batch_size=TRAIN_BS,
        per_device_eval_batch_size=EVAL_BS,
        gradient_accumulation_steps=GRAD_ACCUM,
        learning_rate=LR,
        num_train_epochs=EPOCHS,
        logging_steps=LOG_STEPS,
        eval_strategy="steps" if eval_ds else "no",
        eval_steps=EVAL_STEPS if eval_ds else None,
        save_strategy="steps",
        save_steps=SAVE_STEPS,
        seed=SEED,
        bf16=USE_BF16,
        fp16=False,
        report_to=[],
        gradient_checkpointing=True,
        max_seq_length=MAX_LENGTH if MAX_LENGTH else 2048
    )

    # LoRA ì„¤ì •
    from peft import LoraConfig, TaskType
    peft_cfg = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=[t.strip() for t in LORA_TARGETS.split(",")]
    )

    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ë° í•™ìŠµ
    trainer = SFTTrainer(
        model=model,
        args=sft_config,
        train_dataset=train_ds,
        eval_dataset=eval_ds,
        tokenizer=tok,
        peft_config=peft_cfg,
    )

    print("ğŸš€ í•™ìŠµ ì‹œì‘!")
    trainer.train()
    print("âœ… í•™ìŠµ ì™„ë£Œ!")

    # ìµœì¢… ì–´ëŒ‘í„° ì €ì¥
    print("ğŸ’¾ ìµœì¢… LoRA ì–´ëŒ‘í„° ì €ì¥ ì¤‘...")
    trainer.save_model(OUTPUT_DIR)
    print(f"âœ… LoRA ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: {OUTPUT_DIR}")

    print("\n" + "=" * 60 + "\nğŸ‰ LoRA ì–´ëŒ‘í„° í•™ìŠµ ì™„ë£Œ!\n" + "=" * 60)
    print(f"ğŸ“ ìµœì¢… ì–´ëŒ‘í„° ê²½ë¡œ: {OUTPUT_DIR}")
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. (ì„ íƒ) ì´ ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©í•©ë‹ˆë‹¤.")
    print("2. (ì„ íƒ) ì´ ì–´ëŒ‘í„°ë¥¼ GGUFë¡œ ë³€í™˜í•˜ì—¬ ë² ì´ìŠ¤ GGUFì™€ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print("=" * 60)


if __name__ == "__main__":
    main()
