#!/usr/bin/env python3
"""
ë§¥ë¶ì—ì„œ GGUF ëª¨ë¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
Intel Macê³¼ M1/M2/M3 Mac ëª¨ë‘ ì§€ì›
"""

import subprocess
import sys
import os
from pathlib import Path


def install_llama_cpp_python():
    """llama-cpp-python ì„¤ì¹˜ (Metal ê°€ì† í¬í•¨)"""
    print("ğŸ“¦ llama-cpp-python ì„¤ì¹˜ ì¤‘...")

    # Mì¹© ë§¥ì˜ ê²½ìš° Metal ê°€ì† í™œì„±í™”
    env_vars = os.environ.copy()
    if "arm64" in subprocess.check_output(["uname", "-m"]).decode().lower():
        env_vars["CMAKE_ARGS"] = "-DLLAMA_METAL=on"
        env_vars["FORCE_CMAKE"] = "1"
        print("ğŸš€ Mì¹© ë§¥ ê°ì§€ - Metal ê°€ì† í™œì„±í™”")

    subprocess.run([
        sys.executable, "-m", "pip", "install",
        "llama-cpp-python", "--upgrade", "--force-reinstall", "--no-cache-dir"
    ], env=env_vars)


def run_inference_example():
    """GGUF ëª¨ë¸ë¡œ ì¶”ë¡  ì‹¤í–‰ ì˜ˆì œ"""
    from llama_cpp import Llama

    # GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)
    model_path = "./phi-finetuned-q4_k_m.gguf"  # Q4_K_M ì¶”ì²œ (ì†ë„/í’ˆì§ˆ ê· í˜•)

    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("GGUF íŒŒì¼ì„ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ë°°ì¹˜í•˜ê³  ê²½ë¡œë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.")
        return

    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")

    # ëª¨ë¸ ì´ˆê¸°í™”
    llm = Llama(
        model_path=model_path,
        n_ctx=4096,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
        n_threads=None,  # ìë™ìœ¼ë¡œ ìµœì  ìŠ¤ë ˆë“œ ìˆ˜ ì„¤ì •
        n_gpu_layers=-1,  # Mì¹©: Metal ê°€ì†, Intel: CPUë§Œ ì‚¬ìš©
        verbose=False
    )

    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    # ì¶”ë¡  í…ŒìŠ¤íŠ¸
    while True:
        user_input = input("\nì‚¬ìš©ì ì…ë ¥ (ì¢…ë£Œ: quit): ")
        if user_input.lower() == 'quit':
            break

        # Phi ëª¨ë¸ í¬ë§·ì— ë§ì¶° í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"### Instruction:\n{user_input}\n\n### Response:\n"

        print("ğŸ¤” ìƒì„± ì¤‘...")
        response = llm(
            prompt,
            max_tokens=512,
            temperature=0.7,
            top_p=0.9,
            echo=False,
            stop=["### Instruction:", "### Response:"]
        )

        print(f"ğŸ¤– ì‘ë‹µ: {response['choices'][0]['text'].strip()}")


def benchmark_model():
    """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    from llama_cpp import Llama
    import time

    model_path = "./phi-finetuned-q4_k_m.gguf"

    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return

    print("ğŸš€ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")

    start_time = time.time()
    llm = Llama(model_path=model_path, n_ctx=2048, verbose=False)
    load_time = time.time() - start_time

    # ì¶”ë¡  ì†ë„ í…ŒìŠ¤íŠ¸
    test_prompt = "### Instruction:\nì•ˆë…•í•˜ì„¸ìš”, ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.\n\n### Response:\n"

    start_time = time.time()
    response = llm(test_prompt, max_tokens=100, temperature=0.7)
    inference_time = time.time() - start_time

    tokens_generated = len(response['choices'][0]['text'].split())
    tokens_per_sec = tokens_generated / inference_time

    print(f"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
    print(f"   ëª¨ë¸ ë¡œë”© ì‹œê°„: {load_time:.2f}ì´ˆ")
    print(f"   ì¶”ë¡  ì‹œê°„: {inference_time:.2f}ì´ˆ")
    print(f"   ìƒì„± í† í° ìˆ˜: {tokens_generated}")
    print(f"   í† í°/ì´ˆ: {tokens_per_sec:.2f}")


if __name__ == "__main__":
    print("ğŸ ë§¥OS GGUF ëª¨ë¸ ì‹¤í–‰ê¸°")
    print("=" * 40)

    try:
        import llama_cpp

        print("âœ… llama-cpp-python ì´ë¯¸ ì„¤ì¹˜ë¨")
    except ImportError:
        install_llama_cpp_python()

    while True:
        print("\nì„ íƒí•˜ì„¸ìš”:")
        print("1. ëŒ€í™”í˜• ì¶”ë¡ ")
        print("2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("3. ì¢…ë£Œ")

        choice = input("ì„ íƒ (1-3): ").strip()

        if choice == "1":
            run_inference_example()
        elif choice == "2":
            benchmark_model()
        elif choice == "3":
            break
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")